<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Real2Render2Real</title>
  <link rel="icon" href="./data/favicon.png" type="image/png">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <style>
    :root {
      --accent: #3478f6;
      --bg: #ffffff;
      --text: #111;
      --sidebar-bg: #f8f9fa;
      --sidebar-text: #555;
      --sidebar-active: var(--accent);
      --title-font: 'Inter', system-ui, sans-serif;
      --body-font: 'Inter', system-ui, sans-serif;
      --sidebar-width: max(20vw, 180px);
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      font-family: var(--body-font);
      background-color: var(--bg);
      color: var(--text);
      display: flex;
      scroll-behavior: smooth;
      font-size: 1.08rem;
      line-height: 1.6;
    }

    .sidebar {
      width: var(--sidebar-width);
      position: fixed;
      height: 100vh;
      overflow-y: auto;
      background: #fff;
      border: none;
      padding: 2rem 1rem;
      text-align: right;
    }

    .sidebar h2 {
      font-size: 1.25rem;
      margin-bottom: 1rem;
      color: var(--text);
    }

    .sidebar ul {
      list-style: none;
      padding: 0;
    }

    .sidebar ul li {
      margin-bottom: 0.75rem;
    }

    .sidebar a {
      text-decoration: none;
      color: var(--sidebar-text);
      font-weight: 500;
      transition: color 0.25s, font-weight 0.25s, text-shadow 0.25s;
    }

    .sidebar a.active,
    .sidebar a:hover {
      color: var(--sidebar-active);
      font-weight: 700;
      text-shadow: 0 2px 8px rgba(52,120,246,0.10), 0 1px 0 #fff;
      text-decoration: none;
    }

    main {
      /* margin-left: var(--sidebar-width); */
      margin: auto;
      width: calc(80% - var(--sidebar-width));
      max-width: 1000px;
      padding: 0 2rem;
      box-sizing: border-box;
    }

    section {
      margin-bottom: 3rem;
    }

    h1, h2 {
      color: #1a1a1a;
      font-family: var(--title-font);
      font-weight: 700;
      letter-spacing: -0.5px;
    }

    h1 {
      font-size: 2.1rem;
      margin-bottom: 0.5rem;
    }

    h2 {
      font-size: 1.35rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }

    .main-title {
      font-size: 4rem;
      font-weight: 800;
      font-family: var(--title-font);
      margin-bottom: 0.2rem;
      line-height: 1.1;
      letter-spacing: -1px;
      text-align: left;
      color: #111;
    }

    .main-title .gradient {
      background: linear-gradient(90deg, #cf60d3 0%, #7221ff 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      color: transparent;
      font-weight: 900;
    }

    .subtitle {
      font-size: 1.95rem;
      font-weight: 500;
      color: #222;
      margin-bottom: 1.1rem;
      line-height: 1.3;
      text-align: left;
    }

    .video-carousel {
      display: flex;
      overflow-x: auto;
      gap: 0.75rem;
      padding: 0.5rem 4px;
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      box-sizing: border-box;
      white-space: nowrap;
    }

    .video-carousel video {
      width: 240px;
      border-radius: 4px;
      border: 1px solid #ccc;
      flex-shrink: 0;
    }

    .iframe-pair {
      display: flex;
      flex-wrap: wrap;
      gap: 1rem;
      margin-top: 1rem;
      justify-content: center;
    }

    .iframe-pair iframe {
      flex: 1 1 48%;
      height: 400px;
      border: 1px solid #ccc;
      border-radius: 4px;
    }

    img {
      max-width: 100%;
      border-radius: 4px;
      /* border: 1px solid #ccc; */
    }

    pre {
      background: #f0f0f0;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
      font-size: 0.92em;
    }

    code {
      font-size: 0.92em;
    }

    footer {
      text-align: center;
      font-size: 0.9rem;
      color: #888;
      padding: 1rem;
    }

    @media (max-width: 900px) {
      .sidebar {
        display: none;
      }
      :root {
        --sidebar-width: 0;
      }
      main {
        margin-left: 0;
        width: 100%;
        padding: 0;
      }
      .main-content {
        max-width: 100vw;
        padding: 0 8px;
      }
      .video-carousel {
        max-width: 100vw;
        padding: 0.5rem 8px;
      }
    }

    .main-content {
      max-width: min(1000px, calc(100vw - 240px));
      width: 100%;
      margin: 0 auto;
      overflow: visible;
    }
  </style>
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      const links = document.querySelectorAll('.sidebar a');
      const sections = Array.from(document.querySelectorAll('main section'));

      function updateActiveLink() {
        const scrollY = window.scrollY + 100;
        let current = sections[0];

        for (const section of sections) {
          if (section.offsetTop <= scrollY) {
            current = section;
          }
        }

        links.forEach(link => {
          link.classList.remove('active');
          if (link.getAttribute('href') === `#${current.id}`) {
            link.classList.add('active');
          }
        });
      }

      window.addEventListener('scroll', updateActiveLink);
      updateActiveLink();

      // Iframe carousel logic
      const iframeCarousel = document.getElementById('iframe-carousel');
      const iframeLabel = document.getElementById('iframe-carousel-label');
      const prevBtn = document.getElementById('iframe-carousel-prev');
      const nextBtn = document.getElementById('iframe-carousel-next');
      const iframeData = [
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/tiger_pick_r2r2r_recording_20250507_112639.viser&initialCameraPosition=1.027,0.445,0.681&initialCameraLookAt=0.188,-0.113,-0.129&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Tiger Pick'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/faucet_recording_20250507_110111.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Faucet Turn'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/package_recording_20250507_103622.viser&initialCameraPosition=1.050,0.428,0.670&initialCameraLookAt=0.000,0.000,0.000&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Package Open'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/drawer_reversed_demo.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Drawer Pull'
        },
        {
          src: 'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/mug_demo.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
          label: 'Mug Place'
        }
      ];
      let iframeIdx = 0;
      function updateIframeCarousel() {
        iframeCarousel.src = iframeData[iframeIdx].src;
        iframeLabel.textContent = iframeData[iframeIdx].label;
      }
      if (prevBtn && nextBtn && iframeCarousel && iframeLabel) {
        prevBtn.addEventListener('click', () => {
          iframeIdx = (iframeIdx - 1 + iframeData.length) % iframeData.length;
          updateIframeCarousel();
        });
        nextBtn.addEventListener('click', () => {
          iframeIdx = (iframeIdx + 1) % iframeData.length;
          updateIframeCarousel();
        });
      }

      // Main iframe swap logic for thumbnails
      const mainIframe = document.getElementById('main-iframe');
      const thumbnailVideos = document.querySelectorAll('#iframe-thumbnails video');
      const iframeSrcs = [
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/faucet_recording_20250507_110111.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/cardboard_box_recording_20250507_103622.viser&initialCameraPosition=1.050,0.428,0.670&initialCameraLookAt=0.000,0.000,0.000&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/drawer_recording_20250507_104056.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/replace_this_later.viser&initialCameraPosition=1.050,0.428,0.700&initialCameraLookAt=0.000,0.000,-0.100&initialCameraUp=-0.000,-0.000,1.000',
        'https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/tiger_pick_r2r2r_recording_20250507_112639.viser&initialCameraPosition=1.027,0.445,0.681&initialCameraLookAt=0.188,-0.113,-0.129&initialCameraUp=-0.000,-0.000,1.000',
      ];
      if (mainIframe && thumbnailVideos.length === iframeSrcs.length) {
        thumbnailVideos.forEach((vid, idx) => {
          vid.addEventListener('click', () => {
            mainIframe.src = iframeSrcs[idx];
            // Highlight the selected video
            thumbnailVideos.forEach(v => v.style.outline = '');
            vid.style.outline = '3px solid var(--accent)';
          });
        });
        // Highlight the first by default
        thumbnailVideos[0].style.outline = '3px solid var(--accent)';
      }

      // Physical rollouts main video selector logic
      const mainPhysicalVideo = document.getElementById('main-physical-video');
      const physicalThumbnails = document.querySelectorAll('#physical-video-thumbnails video');
      if (mainPhysicalVideo && physicalThumbnails.length > 0) {
        physicalThumbnails.forEach((vid, idx) => {
          vid.addEventListener('click', () => {
            const src = vid.getAttribute('data-video');
            if (src) {
              mainPhysicalVideo.querySelector('source').src = src;
              mainPhysicalVideo.load();
              mainPhysicalVideo.play();
              // Highlight selected
              physicalThumbnails.forEach(v => v.style.outline = '');
              vid.style.outline = '3px solid var(--accent)';
            }
          });
        });
        // Highlight the first by default
        physicalThumbnails[0].style.outline = '3px solid var(--accent)';
      }
    });
  </script>
</head>
<body>
  <nav class="sidebar">
    
    <ul>
<li><a href="#abstract">Abstract</a></li>
<li><a href="#realrollouts">Real World Demos</a></li>
<li><a href="#scaling">Performance Scaling</a></li>
<li><a href="#reconstruction">Approach</a></li>
<li><a href="#iframes">Embodiment Agnostic Data</a></li>
<li><a href="#randomization">Data Distribution</a></li>
<li><a href="#trajectories">Trajectory Interpolation</a></li>
<li><a href="#bibtex">BibTeX</a></li>
    </ul>
  </nav>
  <main>
    <div class="main-content">
    <section id="title-block">
      <div style="margin-top: 20px;"></div>
      <div class="main-title"><span>Real2</span><span class="gradient">Render</span><span>2Real</span></div>
      <div class="subtitle">Scaling Robotic Manipulation Data Without Dynamics Simulation or Robot Hardware</div>
      <p style="font-size: 1.1rem; margin-bottom: 0.1rem;">Anonymous Authors, Under Peer Review</p>
      <!-- <p style="font-size: 1rem; font-style: italic; margin-bottom: 0.1rem;">Under Review CoRL 2025</p> -->
    </section>

    <section id="overview-video">
      <div style="display: flex; justify-content: center; width: 100%; margin: 1rem 0;">
        <video autoplay muted loop playsinline style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
          <source src="data/twitter-final-compress.mp4" type="video/mp4">
        </video>
      </div>
      <div style="max-width: 900px; margin: 0 auto 1.5rem auto; font-size: 1.15rem; color: #222; text-align: center;">
        <b>Real2Render2Real (R2R2R)</b> is a scalable pipeline for generating data to train generalist manipulation policies - without physics simulation or teleoperation.
      </div>
      <div style="text-align: center; margin-bottom: 2.5rem;">
        <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[Paper (coming soon)]</a>
        <a href="#" style="margin-right: 1.5rem; color: #3478f6; text-decoration: none; font-weight: 600;">[arXiv (coming soon)]</a>
        <a href="#" style="color: #3478f6; text-decoration: none; font-weight: 600;">[Code (coming soon)]</a>
      </div>
    </section>
    <section id="abstract">
      <h1>Abstract</h1> 
        Scaling robot learning requires vast and diverse datasets. Yet
        the prevailing data collection paradigm—human teleoperation—remains costly
        and constrained by manual effort and physical robot access. We introduce
        <b>Real2Render2Real (R2R2R)</b>, a scalable pipeline for generating robot training
        data without physics simulation or teleoperation. Using a smartphone-captured
        scan of one or more objects and a single monocular human demonstration, R2R2R
        reconstructs detailed 3D object geometry and appearance, tracks 6-DoF object motion, and synthesizes thousands of physically plausible, robot-agnostic demonstrations through parallel-environment photorealistic rendering and inverse kinematics.
        Data generated by R2R2R integrates directly with models that operate on
        robot proprioceptive states and image observations, such as vision-language-action
        models (VLA) and imitation learning policies. Physical experiments suggest that
        models trained on R2R2R data alone can achieve comparable performance to those
        trained on teleoperated demonstrations, with model performance scaling with the
        amount and diversity of R2R2R data, while requiring <b>1/27</b> of the time to generate.
        By decoupling data generation from physical robot constraints, R2R2R enables the
        computational scaling of robot datasets to support increasingly capable generalist
        policies.</p>
    </section>

    <section id="The Distinction Between Simulation and Rendering">
      <h3>This is often a point of confusion for our project so here's a section dedicated to clearing it up:</h3>
      <p>  When we refer to <b>simulation</b>, we mean the use of a physics engine to computationally model and simulate dynamic interactions in the environment. In contrast, <b>rendering</b> refers to generating visual data from a graphics engine. In our project, we use IsaacLab, which is commonly used as a physics simulator. However, we disable its dynamics simulation capabilities and instead use it solely as a rendering engine. This allows us to generate high-quality synthetic visual observations and robot kinematic data, which can be used to train imitation learning policies.</p>
      <table>
        <tr>
          
        </tr>
      </table>
    </section>

    <section id="realrollouts">
        <h2>Real World Demos</h2>
        <div style="display: flex; justify-content: center; width: 100%; margin-bottom: 1rem;">
          <video id="main-physical-video" autoplay muted loop playsinline style="width: 100%; max-width: 900px; border-radius: 4px; border: 1px solid #ccc;">
            <source src="data/real_vids/faucet_real_5x_side.mp4" type="video/mp4">
          </video>
        </div>
        <div class="video-carousel" id="physical-video-thumbnails">
          <video data-video="data/real_vids/faucet_real_5x_side.mp4" autoplay muted loop playsinline src="data/real_vids/faucet_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/mug_real_5x_side.mp4" autoplay muted loop playsinline src="data/real_vids/mug_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/franka_real_30x_front.mp4" autoplay muted loop playsinline src="data/real_vids/franka_real_30x_front.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/package_real_5x_side.mp4" autoplay muted loop playsinline src="data/real_vids/package_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/drawer_real_5x_side.mp4" autoplay muted loop playsinline src="data/real_vids/drawer_real_5x_side.mp4" style="cursor:pointer;"></video>
          <video data-video="data/real_vids/tiger_real_5x_side.mp4" autoplay muted loop playsinline src="data/real_vids/tiger_real_5x_side.mp4" style="cursor:pointer;"></video>
        </div>
      </section>

    <section id="scaling">
      <h2>Performance Scaling</h2>
      <div style="display: flex; justify-content: center; width: 100%; margin: 2rem 0;">
        <img src="data/r2r2r_plot.png" alt="Performance Plot" style="max-width: 900px; width: 100%; border-radius: 4px">
      </div>
      <p>Comparative analysis of imitation-learning policies trained on R2R2R-generated data against human teleoperation data across 1050 physical robot experiments and 5 robotic tasks.</p>
    </section>

    <section id="reconstruction">
      <h2>Scan, Track, Render</h2>
      <!-- Main Iframe and Video Thumbnails as Interactive Selector -->
      <div style="display: flex; flex-direction: column; align-items: center; width: 100%;">
        <iframe id="main-iframe" src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/tiger_pick_r2r2r_recording_20250507_112639.viser&initialCameraPosition=1.027,0.445,0.681&initialCameraLookAt=0.188,-0.113,-0.129&initialCameraUp=-0.000,-0.000,1.000" style="width: 100%; max-width: 100%; height: 460px; border: 1px solid #ccc; border-radius: 8px; margin-bottom: 0.75rem; box-shadow: 0 0 8px rgba(0, 0, 0, 0.1);"></iframe>
        <div class="video-carousel" id="iframe-thumbnails">
          <video data-iframe="0" autoplay muted loop playsinline src="data/demo_vids/faucet_demo.mp4" style="cursor:pointer;"></video>
          <video data-iframe="1" autoplay muted loop playsinline src="data/demo_vids/package_demo.mp4" style="cursor:pointer;"></video>
          <video data-iframe="2" autoplay muted loop playsinline src="data/demo_vids/drawer_reversed_demo.mp4" style="cursor:pointer;"></video>
          <video data-iframe="3" autoplay muted loop playsinline src="data/demo_vids/mug_demo.mp4" style="cursor:pointer;"></video>
          <video data-iframe="4" autoplay muted loop playsinline src="data/demo_vids/tiger_demo.mp4" style="cursor:pointer;"></video>
        </div>
      </div>
      <!-- <div style="display: flex; gap: 1rem; width: 100%; margin-top: 1rem;">
        <iframe src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/faucet_recording_20250507_110111.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000" style="flex: 1 1 48%; height: 360px; border: 1px solid #ccc; border-radius: 8px;"></iframe>
        <iframe src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/drawer_reversed_demo_pair.viser&initialCameraPosition=1.026,0.455,0.651&initialCameraLookAt=0.108,-0.143,-0.040&initialCameraUp=-0.000,-0.000,1.000" style="flex: 1 1 48%; height: 360px; border: 1px solid #ccc; border-radius: 8px;"></iframe>
      </div> -->
    </section>

    <section id="iframes">
  <h2>Rendering More Embodiments</h2>
  <p>Part trajectories from a single demonstration can be retargeted across different robot embodiments.</p>
  <div class="iframe-pair">
    <iframe src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/coffee_maker_recording_20250506_191701.viser&initialCameraPosition=1.009,0.430,0.637&initialCameraLookAt=0.240,-0.160,-0.081&initialCameraUp=-0.000,-0.000,1.000" width="480" height="360"></iframe>
    <iframe src="https://uynitsuj.github.io/viser-client/?playbackPath=https://uynitsuj.github.io/recordings/franka_coffee_maker_recording_20250506_210619.viser&initialCameraPosition=0.991,-0.089,0.591&initialCameraLookAt=0.000,0.000,0.000&initialCameraUp=-0.000,-0.000,1.000" width="480" height="360"></iframe>
 </div>
 <div class="iframe-pair">
  <video src="data/real_vids/mug_real_5x_side.mp4" autoplay muted loop playsinline style="max-width: 450px;border-radius: 5px;"></video>
  <video src="data/real_vids/franka_real_30x_front.mp4" autoplay muted loop playsinline style="max-width: 450px;border-radius: 5px;"></video>
 </div>
</section>

    <section id="randomization">
      <h2>Randomization Range</h2>
      <p>We randomize initial object poses to generate diverse synthetic rollouts for each object-task combination.</p>
      <div class="video-carousel">
        <video autoplay muted loop playsinline src="data/randomization/faucet_rand_range.mp4"></video>
        <video autoplay muted loop playsinline src="data/randomization/package_rand_range.mp4"></video>
        <video autoplay muted loop playsinline src="data/randomization/drawer_rand_range.mp4"></video>
        <video autoplay muted loop playsinline src="data/randomization/mug_rand_range.mp4"></video>
        <video autoplay muted loop playsinline src="data/randomization/tiger_rand_range.mp4"></video>
        <video autoplay muted loop playsinline src="data/randomization/franka_rand_range.mp4"></video>
      </div>
    </section>

    <section id="trajectories">
      <h2>Trajectory Interpolation</h2>
      <p>From a single demonstration, R2R2R generates a distribution of plausible trajectories by interpolating 6-DoF part motion.</p>
      <img src="data/traj_interp.png" alt="Trajectory Interpolation" style="border: 1px solid #ccc;">
    </section>

    <section id="bibtex">
      <h2>BibTeX (To be Updated)</h2>
      <pre><code>@article{real2render2real,
  title={Real2Render2Real: Scaling Robotic Manipulation Data Without Dynamics Simulation or Robot Hardware},
  author={real2render2real@gmail.com},
  year={2025},
}</code></pre>
    </section>
    <footer>
      &copy; Webpage designed by the Real2Render2Real team, inspired by the <a href="https://www.videomimic.net/">VideoMimic</a> website.
    </footer>
    </div>
  </main>
</body>
</html>
